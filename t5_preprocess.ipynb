{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import regex as re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('HateSpeechData.csv')\n",
    "tweet = dataset['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(tweet_series):  \n",
    "    tweet_series = tweet_series.astype(str)\n",
    "\n",
    "    tweet_series = tweet_series.str.replace(r'\\s+', ' ', regex=True)\n",
    "    tweet_series = tweet_series.str.replace(r'@[\\w\\-]+', '', regex=True)\n",
    "    tweet_series = tweet_series.str.replace(r'http[s]?://\\S+', '', regex=True)\n",
    "\n",
    "    tweet_series = tweet_series.str.replace(r'[^a-zA-Z]', ' ', regex=True)\n",
    "    tweet_series = tweet_series.str.replace(r'\\s+', ' ', regex=True)\n",
    "    tweet_series = tweet_series.str.replace(r'^\\s+|\\s+?$', '', regex=True)\n",
    "    tweet_series = tweet_series.str.replace(r'\\d+(\\.\\d+)?', 'numbr', regex=True)\n",
    "    tweet_series = tweet_series.str.lower()\n",
    "\n",
    "    tokenized = tweet_series.apply(lambda x: x.split())\n",
    "    # tokenized = tokenized.apply(lambda x: [w for w in x if w not in stopwords])\n",
    "    # tokenized = tokenized.apply(lambda x: [stemmer.stem(w) for w in x])\n",
    "    cleaned = tokenized.apply(lambda x: ' '.join(x))\n",
    "    return cleaned\n",
    "\n",
    "processed_tweets = preprocess(dataset['tweet'])   \n",
    "\n",
    "dataset['processed_tweets'] = processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    processed_tweets\n",
      "0  rt as a woman you shouldn t complain about cle...\n",
      "1  rt boy dats cold tyga dwn bad for cuffin dat h...\n",
      "2  rt dawg rt you ever fuck a bitch and she start...\n",
      "3                          rt she look like a tranny\n",
      "4  rt the shit you hear about me might be true or...\n",
      "5  the shit just blows me claim you so faithful a...\n",
      "6  i can not just sit up and hate on another bitc...\n",
      "7  cause i m tired of you big bitches coming for ...\n",
      "8  amp you might not get ya bitch back amp thats ...\n",
      "9              hobbies include fighting mariam bitch\n"
     ]
    }
   ],
   "source": [
    "print(dataset[[\"processed_tweets\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive_dataset = dataset[dataset[\"class\"] != 2].sample(n=2000, random_state=42)\n",
    "offensive_dataset.to_csv(\"hate_neutral.csv\", index=False, columns=[\"processed_tweets\", \"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive_dataset = dataset[dataset[\"class\"] != 2].sample(n=400)\n",
    "offensive_dataset.to_csv(\"hate_neutral_eval.csv\", index=False, columns=[\"processed_tweets\", \"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 60,801,536 || trainable%: 0.4850\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-t5/t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# --- Apply LoRA ---\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tokenize Dataset ---\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"input_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(dataset))\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m\n\u001b[0;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlora-t5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m---> 18\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtokenized_dataset\u001b[49m,\n\u001b[0;32m     19\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     20\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"lora-t5-small\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=2,\n",
    "    logging_dir=\"logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora-t5small-hate-neutral\")\n",
    "tokenizer.save_pretrained(\"lora-t5small-hate-neutral\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4248",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
